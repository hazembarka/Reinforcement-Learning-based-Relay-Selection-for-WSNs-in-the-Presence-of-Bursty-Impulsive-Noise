{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Importing NumPy for numerical computing, such as array operations and mathematical functions.\n",
    "import matplotlib.pyplot as plt # Importing Pyplot from Matplotlib for data visualization and plotting graphs.\n",
    "from matplotlib import cm # Importing 'cm' from Matplotlib for access to color maps used in plotting.\n",
    "from scipy.special import erfc # Importing 'erfc' (complementary error function) from SciPy's special functions module.\n",
    "import pandas as pd # Importing Pandas for data manipulation and analysis, particularly for structured data operations.\n",
    "from modem import PSKModem, QAMModem, PAMModem, FSKModem # Importing various modem types (Phase Shift Keying, Quadrature Amplitude Modulation, Pulse Amplitude Modulation, Frequency Shift Keying) from a custom 'modem' module.\n",
    "from channels import awgn, rayleighFading, TSMG # Importing functions for simulating different types of channels: Additive White Gaussian Noise, Rayleigh Fading, and TSMG (possibly a custom channel type) from a custom 'channels' module.\n",
    "from errorRates import ser_rayleigh # Importing a function to calculate symbol error rate in Rayleigh fading channel conditions from a custom 'errorRates' module.\n",
    "from tqdm.notebook import tqdm # Importing 'tqdm' from tqdm.notebook for displaying progress bars in Jupyter notebooks.\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the RL-adapted WSN simulation Environment (REINFORCE): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSN_env_RL():\n",
    "\n",
    "####################################################### Initialization ########################################################################\n",
    "\n",
    "    def __init__(self, M=10, Tc = 20,Env_size = 30,Modem_type='QAM',Mod_order = 4,p_B = 0,R = 20,NOISE_MEMORY=20,random_seed = 10):\n",
    "        # Initialization method for the WSN_environment class.\n",
    "        # Parameters:\n",
    "        # M: Number of nodes in the network.\n",
    "        # Tc: Coherence Time in symbol duration.\n",
    "        # Env_size: Size of the environment.\n",
    "        # Modem_type: Type of modem used (PSK, QAM, PAM).\n",
    "        # Mod_order: Modulation order.\n",
    "        # p_B, R, NOISE_MEMORY: Parameters for noise model.\n",
    "        # random_seed: Seed for random number generation.\n",
    "        self.M = M\n",
    "        self.p_B = p_B \n",
    "        self.R = R \n",
    "        self.NOISE_MEMORY = NOISE_MEMORY\n",
    "        self.agent = None\n",
    "        self.node_battery_states = np.ones(M-1)\n",
    "\n",
    "        #Tc/Ts ratio:\n",
    "        self.Tc =  Tc \n",
    "        #Modulation parameters\n",
    "        modem_dict = {'psk': PSKModem,'qam':QAMModem,'pam':PAMModem}\n",
    "        self.Modem = modem_dict[Modem_type.lower()](Mod_order)\n",
    "        self.Mod_order = Mod_order \n",
    "\n",
    "        #nodes and links characteristics \n",
    "        self.Env_size = Env_size \n",
    "        _ = self.reset()\n",
    "        #self.visualize()\n",
    "        #print('Initial link CSIs :')\n",
    "        #print(self.links_CSI.head(10))\n",
    "\n",
    "    def reset(self,n_time_step = 1):\n",
    "        #nodes and links characteristics \n",
    "        self.node_positions = self.init_positions()\n",
    "        self.links_CSI = pd.DataFrame(columns=['link','h','h_abs','G/B'])\n",
    "        self.initiate_links()\n",
    "        self.log = []\n",
    "        return self.training_step(n_time_step = n_time_step)\n",
    "\n",
    "\n",
    "\n",
    "    def init_positions(self):\n",
    "        # Initializes positions of nodes in the network.\n",
    "\n",
    "        while True :\n",
    "\n",
    "            node_positions = np.array([np.random.randint(0,self.Env_size,self.M),np.random.randint(0,self.Env_size,self.M)]).T\n",
    "            node_positions[0] = np.array([0,self.Env_size-1])\n",
    "            node_positions[-1] = np.array([self.Env_size-1,0])\n",
    "            found_positions = True\n",
    "            for i in range(len(node_positions)):\n",
    "                for j in range(len(node_positions)):\n",
    "                    if j!=i and node_positions[i][0] == node_positions[j][0] and node_positions[i][1] == node_positions[j][1]:\n",
    "                        found_positions = False\n",
    "            if found_positions:\n",
    "                break\n",
    "        return node_positions\n",
    "\n",
    "\n",
    "    def initiate_links(self): # Initializes the links between nodes and their channel state information (CSI).\n",
    "        self.links_CSI = pd.DataFrame(columns=['link','distance','h','h_abs','G/B'])\n",
    "        index = 0\n",
    "        for i in range(1,self.M+1):\n",
    "            for j in range(1,self.M+1):\n",
    "                if i !=j :\n",
    "                    d = self.distance(i,j)\n",
    "                    h = np.array(rayleighFading(1,d,2)[0],dtype = 'complex_')\n",
    "                    self.links_CSI.loc[index] = [str(i)+'->'+str(j),d,h,abs(h),'G']\n",
    "                    index+=1\n",
    "\n",
    "\n",
    "    def visualize(self):\n",
    "        print('visualize  environment :')\n",
    "\n",
    "        for i, txt in enumerate(np.arange(1,self.M)):\n",
    "            for j in range(1,self.M):\n",
    "                plt.annotate(\"\", xy=(self.node_positions[i,0], self.node_positions[i,1]), xytext=(self.node_positions[j,0], self.node_positions[j,1]),\n",
    "                arrowprops=dict(arrowstyle=\"<->\"))\n",
    "            \n",
    "        for i, txt in enumerate(np.arange(1,self.M)):\n",
    "            plt.annotate(txt, (self.node_positions[i,0]+0.1, self.node_positions[i,1]+0.1),color='green',size=20)\n",
    "        plt.annotate('D', (self.node_positions[-1,0]+0.1, self.node_positions[-1,1]+0.1),color='blue',size=20)\n",
    "\n",
    "        plt.scatter(self.node_positions[:,0], self.node_positions[:,1],\n",
    "            c='g', alpha=0.6, lw=0,s=400)\n",
    "        plt.scatter(self.node_positions[-1,0], self.node_positions[-1,1], c='g', alpha=0.6, lw=0,s=400  )\n",
    "        plt.xlim(0,self.Env_size)\n",
    "        plt.ylim(0,self.Env_size)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################### Helpfull functions ########################################################################\n",
    "\n",
    "    def update_all_links(self):\n",
    "        # Updates the channel state information (CSI) for all links in the network.\n",
    "        # This is done by first setting the 'h' column in the links_CSI DataFrame equal to the distance values.\n",
    "        # Then, it applies the `update_one_link` function to each 'h' value to update the CSI.\n",
    "        # Finally, it calculates the absolute value of each 'h' and stores it in 'h_abs'.\n",
    "        self.links_CSI['h'] =  self.links_CSI['distance'].values\n",
    "        self.links_CSI['h'] =  self.links_CSI['h'].apply(self.update_one_link)\n",
    "        self.links_CSI['h_abs'] =  self.links_CSI['h'].apply(np.abs)\n",
    "\n",
    "\n",
    "\n",
    "    def update_one_link(self, d):\n",
    "        # Updates the channel state information for a single link.\n",
    "        # This function takes the distance 'd' as input and returns the CSI as a complex number.\n",
    "        # The rayleighFading function is used to model the fading effect on the link based on the distance.\n",
    "        # The output is formatted as a complex number to represent the amplitude and phase shift caused by the fading.\n",
    "        return np.array(rayleighFading(1,d,2)[0],dtype = 'complex_')\n",
    "\n",
    "\n",
    "    def distance(self, node1, node2):\n",
    "        # Calculates the normalized distance between two nodes in the network.\n",
    "        # The function computes the Euclidean distance between the positions of node1 and node2.\n",
    "        # The distance is then normalized with respect to the distance between nodes 1 and 10 in the network.\n",
    "        # This normalization might be specific to the network layout or design criteria.\n",
    "        distance_1_to_10 =  np.linalg.norm(self.node_positions[0] - self.node_positions[-1],ord=2) \n",
    "        return  np.linalg.norm(self.node_positions[node1-1] - self.node_positions[node2-1],ord=2)/distance_1_to_10\n",
    "\n",
    "\n",
    "    def training_step(self,EsN0dB_ = 50,n_time_step = 128,actions = np.zeros(128),source = 1,allow_battery_consumption = False):\n",
    "        \"\"\"\n",
    "        Actions : array with size nA containing nA action: an action for each frame\n",
    "        action = select a relay\n",
    "        \"\"\"\n",
    "        #k = np.log2(self.Mod_order)\n",
    "        #EsN0dB = 10*np.log10(k)+EbN0dB # EsN0dB calculation\n",
    "        if sum(actions)==0:\n",
    "            actions = np.zeros(n_time_step)\n",
    "            \n",
    "        nSym = n_time_step*self.Tc\n",
    "        #print('nSym :',nSym)\n",
    "\n",
    "        ###############################################  #recover the selected signals: #########################################################\n",
    "\n",
    "        if len(self.log) >0:\n",
    "                \n",
    "                list_optimal_actions = []\n",
    "                list_obtained_SER_MRC = []\n",
    "                list_optimal_MRC_SER = []\n",
    "                list_DT_SER = []\n",
    "                for time_step in range(n_time_step):\n",
    "                    best_relay = int(actions[time_step])  # 0 -> M-3 (a total of M-2 relay) to 2 -> M-1 (1 = source, M = Destination)  \n",
    "\n",
    "                    optimal_MRC_SER = np.min(self.log[time_step]['SERs'])\n",
    "                    optimal_action = np.argmin(self.log[time_step]['SERs'])\n",
    "                    obtained_MRC_SER = np.min(self.log[time_step]['SERs'][best_relay])\n",
    "                    list_optimal_actions.append(optimal_action)\n",
    "                    list_obtained_SER_MRC.append(obtained_MRC_SER)\n",
    "                    list_optimal_MRC_SER.append(optimal_MRC_SER)\n",
    "                    list_DT_SER.append(self.log[time_step]['SER_DT'])\n",
    "                    \n",
    "                    if allow_battery_consumption:\n",
    "                        self.node_battery_states[best_relay+1] -=  self.Tc\n",
    "\n",
    "                    #print('max MRC = max RD_mask ? ',self.log[time_step]['SERs'][np.argmax(self.log[time_step]['RD_masks'])]== optimal_MRC_SER)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "                list_optimal_actions = np.zeros(n_time_step)\n",
    "                list_obtained_SER_MRC = np.ones(n_time_step)\n",
    "                list_optimal_MRC_SER = np.ones(n_time_step)\n",
    "                list_DT_SER = np.ones(n_time_step)\n",
    "        \n",
    "\n",
    "        ##########################################        #Obtaining Next State:        ##############################################################\n",
    "\n",
    "              \n",
    "        self.log = np.zeros(n_time_step,dtype=object)\n",
    "        \n",
    "        next_states_tensor = []\n",
    "        for time_step in range(n_time_step):\n",
    "            state = []\n",
    "            self.log[time_step] = {}\n",
    "            self.update_all_links()\n",
    "            \n",
    "            #uniform random symbols from 0 to M-1 with size nSym\n",
    "            input_syms_chunk = np.random.randint(low=0, high = self.Mod_order, size=self.Tc)\n",
    "            \n",
    "            PGs = []\n",
    "            SERs = []\n",
    "            RD_masks = []\n",
    "\n",
    "            #Sending information from S to D\n",
    "            SD_receivedSyms,h_SD,noise_states_SD = self.send_over_one_link('awgn',input_syms_chunk,source = source,destination = self.M,EsN0dB = EsN0dB_)\n",
    "            SD_receivedSyms_equalized = np.array(h_SD.conjugate()*SD_receivedSyms/abs(h_SD),dtype = 'complex_')\n",
    "            SD_detectedSyms = self.Modem.demodulate(SD_receivedSyms_equalized) #demodulate \n",
    "            SER_DT = np.sum(SD_detectedSyms.astype(int) != input_syms_chunk)/len(input_syms_chunk)\n",
    "            self.log[time_step]['SER_DT'] = SER_DT\n",
    "\n",
    "            #Sending information from S to all possible Relays\n",
    "            for relay in range(1,self.M):\n",
    "                if relay != source:\n",
    "                    receivedSyms,h_SR,SR_noise_states = self.send_over_one_link('tsmg',input_syms_chunk,source = source,destination = relay,EsN0dB = EsN0dB_)\n",
    "                    receivedSyms = np.array(h_SR.conjugate()*receivedSyms/abs(h_SR),dtype = 'complex_')\n",
    "                    SR_detectedSyms = self.Modem.demodulate(receivedSyms) #demodulate \n",
    "\n",
    "                    #Sending information from R to D\n",
    "                    RD_receivedSyms,h_RD,noise_states_RD = self.send_over_one_link('awgn',SR_detectedSyms,source = relay,destination = self.M,EsN0dB = EsN0dB_)\n",
    "\n",
    "\n",
    "                    #Filtering the bad states :\n",
    "                    RD_mask = SR_noise_states*(SR_detectedSyms == input_syms_chunk)\n",
    "            \n",
    "\n",
    "                    #Maximum ratio combiner :\n",
    "                    D_receivedSyms = (h_SD.conjugate()*SD_receivedSyms +  h_RD.conjugate()*RD_mask*RD_receivedSyms) /np.linalg.norm([h_RD,h_SD],ord=2)\n",
    "                    D_receivedSyms_eq = np.array(D_receivedSyms,dtype = 'complex_')\n",
    "                    D_detectedSyms = self.Modem.demodulate(D_receivedSyms_eq) #demodulate \n",
    "                    SER_MRC = np.sum(D_detectedSyms.astype(int) != input_syms_chunk)/len(input_syms_chunk)\n",
    "                    \n",
    "                    p_G= sum(SR_noise_states)/self.Tc\n",
    "\n",
    "                    PGs.append(p_G)\n",
    "                    SERs.append(SER_MRC)\n",
    "                    RD_masks.append(sum(RD_mask))\n",
    "\n",
    "                    state.append(sum(RD_mask)*10/len(RD_mask) - 9)\n",
    "                    state.append(min(np.abs(h_RD),np.abs(h_SR)))\n",
    "                    state.append(np.abs(h_RD)) \n",
    "                    state.append(np.abs(h_SR))\n",
    "                    state.append((1-p_G-0.1)*10)\n",
    "                    state.append(p_G)\n",
    "                    state.append(1-p_G)\n",
    "                    #RBP = 10*(self.node_battery_states[relay-1]-np.min(self.node_battery_states[1:]))/max(self.node_battery_states[1:]) \n",
    "                    #state.append(RBP)\n",
    "                   # state.append(SER_MRC*self.Tc)\n",
    "\n",
    "\n",
    "                   \n",
    "            self.log[time_step]['SERs'] = SERs\n",
    "            self.log[time_step]['PGs'] = PGs\n",
    "            self.log[time_step]['RD_masks'] = RD_masks\n",
    "\n",
    "\n",
    "                     \n",
    "   \n",
    "\n",
    "            next_states_tensor.append(state)\n",
    "\n",
    "        next_states_tensor = torch.Tensor(np.array(next_states_tensor))\n",
    "        \n",
    "        return np.array(list_optimal_actions),np.array(list_obtained_SER_MRC),np.array(list_optimal_MRC_SER),np.array(list_DT_SER),next_states_tensor,False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def send_over_one_link(self, noise_type, inputSyms, source=1, destination=2, EsN0dB=5):\n",
    "        # Simulates the transmission of symbols over a single link between two nodes.\n",
    "        # noise_type: Type of noise to be added to the link (e.g., 'awgn' for Additive White Gaussian Noise, 'tsmg' for another type).\n",
    "        # inputSyms: Array of input symbols to be transmitted.\n",
    "        # source: The source node for the transmission.\n",
    "        # destination: The destination node for the transmission.\n",
    "        # EsN0dB: Energy per symbol to noise power spectral density ratio, in decibels.\n",
    "\n",
    "        # Modulation of the input symbols.\n",
    "        modulatedSyms = self.Modem.modulate(inputSyms)\n",
    "\n",
    "        # Retrieving the channel coefficient 'h' for the specified link.\n",
    "        h = self.links_CSI[self.links_CSI.link == str(source)+'->'+str(destination)]['h'].to_numpy()[0]\n",
    "\n",
    "        # Applying fading effect to the modulated symbols.\n",
    "        fadedSyms = h * modulatedSyms\n",
    "\n",
    "        # Adding noise based on the specified noise type.\n",
    "        if noise_type.lower() == 'awgn':\n",
    "            # Adding Additive White Gaussian Noise.\n",
    "            receivedSyms = awgn(fadedSyms, EsN0dB)\n",
    "            noise_states = []\n",
    "\n",
    "        elif noise_type.lower() == 'tsmg':\n",
    "            # Adding noise modeled by the TSMG function, potentially representing a different noise model.\n",
    "            receivedSyms, noise_states = TSMG(fadedSyms, P_B=self.p_B, R=self.R, NOISE_MEMORY=self.NOISE_MEMORY, SNRdB=EsN0dB)\n",
    "\n",
    "        # Return the received symbols, the channel coefficient, and the noise states.\n",
    "        return receivedSyms, h, noise_states\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6600,  0.3714,  0.3714,  1.1963, -0.6600,  0.9660,  0.0340, -2.7600,\n",
       "          2.0700,  2.0700,  5.0923,  2.7600,  0.6240,  0.3760,  0.0200,  0.8852,\n",
       "          1.3026,  0.8852, -0.0200,  0.9020,  0.0980,  0.3800,  1.6092,  3.1514,\n",
       "          1.6092, -0.3800,  0.9380,  0.0620,  1.0000,  0.5706,  0.5706,  9.9406,\n",
       "         -1.0000,  1.0000,  0.0000, -3.4000,  1.3866,  1.3866,  2.8093,  3.4000,\n",
       "          0.5600,  0.4400, -0.1900,  1.5229,  4.8637,  1.5229,  0.1900,  0.8810,\n",
       "          0.1190,  1.0000,  1.1669,  1.1669,  1.6149, -1.0000,  1.0000,  0.0000]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = WSN_env_RL(M = 10,Modem_type='QAM', Tc = 1000, Mod_order=4,Env_size=40,p_B=0.1, R=100, NOISE_MEMORY=100)\n",
    "_,_,_,_,next_states,done = env.reset(n_time_step=1)\n",
    "next_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the RL Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "**Input:** a differentiable policy network $\\pi_\\theta \\in \\mathcal{R}^d$ \\\\\n",
    "**Algorithm parameters:** \n",
    "\n",
    "\n",
    "1.   $\\alpha$: step size > 0\n",
    "2.   n_iterations: number of gradient updates  > 0\n",
    "3.   n_episodes: number of episodes per gradient update  > 0 \n",
    "\n",
    "Initialize policy parameters \n",
    "\n",
    "loop for n_iterations: \\\\\n",
    "\n",
    "  &emsp;&emsp;sample a dataset of episodes according to $\\pi_\\theta$ \\\\\n",
    "  \n",
    "  &emsp;&emsp;# compute policy gradient \\\\\n",
    "  &emsp;&emsp;$\\nabla_\\theta J(\\theta) = \\sum_j \\sum_t \\psi_{jt} \\nabla_\\theta ln\\pi_\\theta (a_t^j|s_t^j)$ \\\\\n",
    "  \n",
    "  &emsp;&emsp; # where the first summation is over the episodes and the second summation is over the trajectory of the episode. \\\\\n",
    "  \n",
    "  &emsp;&emsp;# update policy parameters \\\\\n",
    "  &emsp;&emsp;$\\theta_i = \\theta_i + \\alpha \\nabla_\\theta J(\\theta)$\n",
    "  \n",
    "------------------------------------------------------------------------------\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# torch stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "# data manipulation, colab dispaly, and plotting\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# misc util\n",
    "import  itertools\n",
    "import time\n",
    "from torch.distributions import Categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Base Agent Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent(object):\n",
    "    \"\"\" The base agent class function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            config: configuration dictionary\n",
    "        \"\"\"\n",
    "        \n",
    "        self.config = config\n",
    "        # assert len(config['policy_layers']) > 0 # this won't allow linear models\n",
    "\n",
    "        # environments\n",
    "        self.env  = WSN_env_RL(M = 10,Modem_type='QAM', Tc = 1000, Mod_order=4,Env_size=40,p_B=0.1, R=100, NOISE_MEMORY=100)\n",
    "        #self.test_env  = WSN_env_RL(M = 10,Modem_type='QAM', Tc = 1000, Mod_order=4,Env_size=40,p_B=0.1, R=100, NOISE_MEMORY=100)\n",
    "        self.env.observation_space_size = (self.env.M-2) * 7\n",
    "        self.env.action_space_size      = self.env.M-2\n",
    "        self.gamma = config['gamma']\n",
    "\n",
    "        # set seed\n",
    "        np.random.seed(seed=config['seed'])\n",
    "        torch.manual_seed(config['seed'])\n",
    "\n",
    "        # build policy model\n",
    "        _policy_logits_model = Model(\n",
    "            [self.env.observation_space_size] + \n",
    "            config['policy_layers'] + # note that these are only the intermediate layers\n",
    "            [self.env.action_space_size],\n",
    "        )\n",
    "        \n",
    "        # NOTE: by design, policy model should take *batches* of states as input.\n",
    "        # self.policy_model spits out the probability of each action\n",
    "        self.policy_model = nn.Sequential(\n",
    "            _policy_logits_model, nn.Softmax(dim=1), \n",
    "        )\n",
    "\n",
    "        self.policy_optimizer = torch.optim.Adam(\n",
    "            self.policy_model.parameters(), \n",
    "            lr=config['policy_learning_rate'],\n",
    "        )\n",
    "\n",
    "    def _make_returns(self, rewards: np.ndarray):\n",
    "        \"\"\" Compute the cumulative discounted rewards at each time step\n",
    "        args:\n",
    "            rewards: an array of step rewards\n",
    "\n",
    "        returns:\n",
    "            returns: an array of discounted returns from that timestep onward\n",
    "        \"\"\"\n",
    "        returns = np.zeros_like(rewards)\n",
    "        returns[-1] = rewards[-1]\n",
    "        for t in reversed(range(len(rewards) - 1)):\n",
    "            returns[t] = rewards[t] + self.gamma * returns[t + 1]\n",
    "        return returns\n",
    "    \n",
    "    \n",
    "    def train(self ,n_episodes: int, n_iterations: int, plot: bool = True,EsN0dB = 5) -> Sequence[np.ndarray]:\n",
    "        \"\"\" Train.\n",
    "        args:\n",
    "            n_episodes: number of episodes for each gradient step\n",
    "            n_iterations: determine training duration\n",
    "        \"\"\"        \n",
    "        max_accuracy = 0\n",
    "        ##first steps for the agent \n",
    "        _,_,_,_,next_states,done = self.env.reset(n_time_step = 1)\n",
    "\n",
    "        self.env.visualize()\n",
    "        _,_,_,_,next_states,done =self.env.training_step(n_time_step = 1,EsN0dB_=EsN0dB)\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        rewards = []\n",
    "        \n",
    "        for it in tqdm(range(n_iterations)):\n",
    "\n",
    "            reward,accuracy,loss,NRBP =  self.optimize_model(n_episodes,1,EsN0dB,next_states)            \n",
    "\n",
    "\n",
    "\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.policy_optimizer.step()\n",
    "\n",
    "            rewards.append(np.array(reward))\n",
    "            accuracies.append(accuracy)\n",
    "            losses.append(loss.detach().numpy().round(2))\n",
    "\n",
    "            if accuracy > max_accuracy and NRBP <1:\n",
    "                torch.save(self.policy_model,'best_model.pth')\n",
    "                max_accuracy = accuracy\n",
    "            print('Iteration ',it + 1,'/',n_iterations,': rewards ',np.mean(rewards[-10:]).round(2),' ## accuracy ',accuracy,' ## policy loss ',loss.detach().numpy().round(2),' ## NRBP ',round(NRBP, 2))\n",
    "\n",
    "        if plot:\n",
    "            self.plot_rewards(rewards)\n",
    "            plt.show()\n",
    "            plt.plot([np.mean(i) for i in rewards ],marker='o')\n",
    "            plt.xlabel(\"episodes\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.show()\n",
    "        \n",
    "            plt.plot(accuracies,marker='o')\n",
    "            plt.ylabel(\"accuracy\")\n",
    "            plt.xlabel(\"episodes\")\n",
    "            plt.show()          \n",
    "            plt.plot(losses,marker='o')\n",
    "            plt.xlabel(\"episodes\")\n",
    "            plt.ylabel(\"policy loss\")\n",
    "            plt.show()\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def plot_rewards(rewards: Sequence[np.ndarray], ax: Optional[Any] = None):\n",
    "        # Plotting\n",
    "        r = pd.DataFrame((itertools.chain(*(itertools.product([i], rewards[i]) for i in range(len(rewards))))), columns=['Epoch', 'Reward'])\n",
    "        if ax is None:\n",
    "            sns.lineplot(x=\"Epoch\", y=\"Reward\", data=r, ci='sd');\n",
    "        else:\n",
    "            sns.lineplot(x=\"Epoch\", y=\"Reward\", data=r, ci='sd', ax=ax);\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code and run this cell\n",
    "class REINFORCEv1Agent(BaseAgent):\n",
    "    \"\"\" REINFORCE agent with total trajectory reward.\n",
    "    \"\"\"\n",
    "    \n",
    "    def optimize_model(self, n_episodes,n_iter_per_episode,EsN0dB,next_states):\n",
    "        \"\"\"\n",
    "            This method is called at each training iteration and is responsible for \n",
    "            (i) gathering a dataset of episodes\n",
    "            (ii) computing the expectation of the policy gradient.\n",
    "        \"\"\"\n",
    "        \n",
    "        # ======================================================================\n",
    "        # ======================================================================\n",
    "        # ==========              Training          ============================\n",
    "        # ======================================================================\n",
    "        # ======================================================================\n",
    "\n",
    "        ### (i) gathering a dataset of episodes\n",
    "        accuracy = 0\n",
    "        rewards = torch.zeros((1),requires_grad=False)\n",
    "        log_probs = torch.zeros((1),requires_grad=True)\n",
    "        NRBPVs = []\n",
    "        MRC = []\n",
    "        #OPt_MRC = []\n",
    "        #for each time step\n",
    "        for episode in range(n_episodes):\n",
    "    \n",
    "\n",
    "            probs = self.policy_model(next_states)\n",
    "\n",
    "            ## Calculating the variance in the remaining battery power:\n",
    "            battery_diff = (self.env.node_battery_states[1:]-np.min(self.env.node_battery_states[1:]))/np.max(self.env.node_battery_states[1:])\n",
    "            NRBPV = (np.max(self.env.node_battery_states[1:])-np.min(self.env.node_battery_states[1:]))/np.max(self.env.node_battery_states[1:])\n",
    "            NRBPVs.append(NRBPV)\n",
    "            \n",
    "            all_actions = torch.topk(probs, self.env.M-2).indices\n",
    "            actions = all_actions[:,0]\n",
    "            for i in range(self.env.M-2):\n",
    "                if battery_diff[all_actions[0,i]] > 0.2 * NRBPV:\n",
    "                    actions = all_actions[:,i]\n",
    "                    break\n",
    "\n",
    "            dist = Categorical(probs)\n",
    "            log_probs_ep = dist.log_prob(actions)\n",
    "\n",
    "\n",
    "            #take a step\n",
    "            list_optimal_actions,list_obtained_SER_MRC,list_optimal_MRC_SER,list_DT_SER, next_states,done = self.env.training_step(n_time_step = n_iter_per_episode,actions=actions,EsN0dB_=EsN0dB,allow_battery_consumption = True)\n",
    "            MRC.append(list_obtained_SER_MRC)\n",
    "            #OPt_MRC.append(list_optimal_MRC_SER)\n",
    "\n",
    "            ## Calculating the accuracy of choosing an optimal action :\n",
    "           # print(\"list_optimal_MRC_SER :\",list_optimal_MRC_SER,\"  list_obtained_SER_MRC :\",list_obtained_SER_MRC)\n",
    "            accuracy += sum(list_optimal_MRC_SER == list_obtained_SER_MRC)\n",
    "\n",
    "            NRBPVs.append(NRBPV)\n",
    "\n",
    "            ## Calculating the reward:\n",
    "            reward = -1000*list_obtained_SER_MRC +5\n",
    "                \n",
    "            rewards =  torch.cat([rewards, torch.Tensor(reward)], axis=0)\n",
    "            log_probs =  torch.cat([log_probs, log_probs_ep], axis=0)\n",
    "            \n",
    "            #print(\"rewards.shape : \", rewards.shape)\n",
    "            #print(\"log_probs.shape : \", log_probs.shape)\n",
    "\n",
    "        accuracy = accuracy/n_episodes\n",
    "        rewards = rewards[1:]\n",
    "        log_probs = log_probs[1:]\n",
    "\n",
    "      \n",
    "        # ========================================================================================================================================\n",
    "        \n",
    "        policy_loss =  -torch.sum(rewards * log_probs)\n",
    "        \n",
    "        #print(' MRC ',np.mean(MRC).round(5),' OPT MRC ',np.mean(OPt_MRC).round(5),'  #', end='')\n",
    "\n",
    "        return rewards.detach().numpy().round(2),accuracy.round(2),policy_loss,np.mean(NRBPVs)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, features: Sequence[int]):\n",
    "        \"\"\"Fully-connected Network\n",
    "\n",
    "        Args:\n",
    "            features: a list of ints like: [input_dim, 16, 16, output_dim]\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(len(features) - 1):\n",
    "            layers.append(\n",
    "                nn.Linear(\n",
    "                    in_features=features[i],\n",
    "                    out_features=features[i + 1],\n",
    "                    )\n",
    "            )\n",
    "            if i != len(features) - 2:\n",
    "                layers.append(nn.ReLU())\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.net(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'seed': 10,\n",
    "    'gamma': 1.0,\n",
    "    'policy_layers': [64,16],\n",
    "    'policy_learning_rate': 1e-3,\n",
    "    #'value_layers': [128],\n",
    "    #'value_learning_rate': 1e-3,\n",
    "    'use_baseline': False,\n",
    "}\n",
    "\n",
    "\n",
    "agent = REINFORCEv1Agent(config)\n",
    "agent.env.battery_capacity =   agent.env.Tc * 20 * 265 * 150\n",
    "agent.env.node_battery_states = np.ones(agent.env.M-1) *  agent.env.battery_capacity\n",
    "agent.train(n_episodes=128, n_iterations= 50,EsN0dB=5+10*np.log10(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEv1Agent(REINFORCEv1Agent):\n",
    "\n",
    "    def test_model(self, n_episodes,n_iter_per_EsN0dB,EsN0dB,next_states):\n",
    "        \n",
    "        # ======================================================================\n",
    "        # ======================================================================\n",
    "        # ==========              Testing          ============================\n",
    "        # ======================================================================\n",
    "        # ======================================================================\n",
    "        self.policy_model = torch.load('best_model.pth')\n",
    "        self.policy_model.eval()\n",
    "        total_rewards = []\n",
    "        avg_SER_DT = []\n",
    "        avg_optimal_SER_MRC = []\n",
    "        avg_SER_MRC = []\n",
    "        n_time_step = n_iter_per_EsN0dB\n",
    "\n",
    "        #for each time step\n",
    "        for episode in range(n_episodes):\n",
    "    \n",
    "            #select an action using the policy model\n",
    "            probs = self.policy_model(next_states)\n",
    "            actions = torch.argmax(probs,dim = 1)\n",
    "            #print(actions)\n",
    "            \n",
    "            #take a step\n",
    "            _,list_obtained_SER_MRC,list_optimal_MRC_SER,list_DT_SER, next_states,done = self.env.training_step(n_time_step = n_time_step,actions=actions,EsN0dB_=EsN0dB,allow_battery_consumption = True)\n",
    "            \n",
    "            #rewards = 10*(list_SER_DT - list_SER_MRC)/np.mean(list_SER_DT + 0.000000000001)\n",
    "        \n",
    "            #total_rewards.append(np.sum(rewards))\n",
    "            avg_SER_MRC.append(np.mean(list_obtained_SER_MRC))\n",
    "            avg_optimal_SER_MRC.append(np.mean(list_optimal_MRC_SER))\n",
    "            avg_SER_DT.append(np.mean(list_DT_SER))\n",
    "\n",
    "            #total_rewards = np.array(total_rewards)\n",
    "\n",
    "        return total_rewards,np.mean(avg_SER_MRC).round(5),np.mean(avg_optimal_SER_MRC).round(5),np.mean(avg_SER_DT).round(5)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    def train_and_get_SER(self, n_episodes = 10,\n",
    "                            n_iter_per_EsN0dB = 1,\n",
    "                            min_EbN0dB = 5,\n",
    "                            max_EbN0dB = 6,\n",
    "                            EbN0dB_step = 1,\n",
    "                            n_training_iter = 25):\n",
    "\n",
    "\n",
    "\n",
    "        EbN0dBs = range(min_EbN0dB,max_EbN0dB+1,EbN0dB_step)\n",
    "        k = np.log2(self.env.Mod_order)\n",
    "        EsN0dBs = 10*np.log10(k)+EbN0dBs # EsN0dB calculation\n",
    "        \n",
    "        self.env.battery_capacity = len(EbN0dBs)  * self.env.Tc * n_episodes * 2 * n_training_iter\n",
    "        self.env.node_battery_states = np.ones(self.env.M-1) *  self.env.battery_capacity\n",
    "        SER_sim_MRC = []\n",
    "        SER_sim_optimal_MRC = []\n",
    "        SER_sim_DT = []\n",
    "\n",
    "        ##first steps for the agent \n",
    "        _,_,_,_,next_states,done = self.env.reset(n_time_step = n_iter_per_EsN0dB)\n",
    "        #self.env.visualize()\n",
    "        _,_,_,_,next_states,done =self.env.training_step(n_time_step = n_iter_per_EsN0dB,EsN0dB_=EsN0dBs[0])\n",
    "\n",
    "\n",
    "        for EsN0dB in tqdm(EsN0dBs):\n",
    "            _  = agent.train(n_episodes=n_episodes, n_iterations= n_training_iter,EsN0dB =EsN0dB,plot=False)\n",
    "            total_rewards,SER_MRC,SER_optimal_MRC,SER_DT = self.test_model(n_episodes,n_iter_per_EsN0dB,EsN0dB,next_states)\n",
    "            SER_sim_MRC.append(SER_MRC)\n",
    "            SER_sim_optimal_MRC.append(SER_optimal_MRC)\n",
    "            SER_sim_DT.append(SER_DT)\n",
    "\n",
    "\n",
    "        colors = plt.cm.jet(np.linspace(0,1,12)) # colormap\n",
    "        fig, ax = plt.subplots(nrows=1,ncols = 1)\n",
    "        fig.set_size_inches(10, 6)\n",
    "\n",
    "            \n",
    "            \n",
    "        ax.semilogy(EbN0dBs,SER_sim_DT,color = colors[3],marker='o',linestyle='-',label='DT AWGN',markersize = 15)\n",
    "        ax.semilogy(EbN0dBs,SER_sim_MRC,color = colors[1],marker='o',linestyle='-',label='MRC TSMG using REINFORCE',markersize = 15)\n",
    "        ax.semilogy(EbN0dBs,SER_sim_optimal_MRC,color = colors[4],marker='o',linestyle='-',label='Optimal MRC TSMG',markersize = 15)\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        df['EbN0dBs'] = EbN0dBs\n",
    "        df['SER_sim_MRC_TSMG'] = SER_sim_MRC\n",
    "        df['SER_sim_SD_TSMG'] = SER_sim_DT\n",
    "        df.to_csv('Reinforce    .csv',index = False)\n",
    "\n",
    "        ax.set_xlabel('Eb/N0(dB)',fontsize=15);ax.set_ylabel('SER ($P_s$)',fontsize=15)\n",
    "        ax.set_title('Probability of Symbol Error over Rayleigh flat fading channel',fontsize=15)\n",
    "        ax.legend()\n",
    "        ax.xaxis.set_tick_params(labelsize=20)\n",
    "        ax.yaxis.set_tick_params(labelsize=20)\n",
    "        ax.grid(True)\n",
    "        ax.set_ylim([0.00006, 0])\n",
    "\n",
    "        fig, ax = plt.subplots(figsize = (10,6))\n",
    "        print( self.env.node_battery_states[1:self.env.M-1])\n",
    "        remaining_battery_level = self.env.node_battery_states[1:self.env.M-1]*100/  self.env.battery_capacity\n",
    "        print( remaining_battery_level)\n",
    "\n",
    "        #clrs = ['green' if (x > 20) else 'red' for x in remaining_battery_level ]\n",
    "        barlist=ax.bar([ str(i+2) for i in np.arange(self.env.M-2)],remaining_battery_level)\n",
    "        print(remaining_battery_level)\n",
    "\n",
    "        for i in range(len(remaining_battery_level)):\n",
    "            if abs(remaining_battery_level[i])<20:\n",
    "                barlist[i].set_color('r')\n",
    "            else:\n",
    "                barlist[i].set_color('g')\n",
    "\n",
    "        ax.set_title(\"Remaining Battery level for each node\",fontsize = 20)\n",
    "        ax.set_xlabel(\"nodes\",fontsize = 20)\n",
    "        ax.set_ylabel(\"Battery level (%)\",fontsize = 20)\n",
    "        ax.xaxis.set_tick_params(labelsize=20)\n",
    "        ax.yaxis.set_tick_params(labelsize=20)\n",
    "        ax.grid(True)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'seed': 15,\n",
    "    'gamma': 1.0,\n",
    "    'policy_layers': [1024],\n",
    "    'policy_learning_rate': 5e-3,\n",
    "}\n",
    "\n",
    "agent = REINFORCEv1Agent(config)\n",
    "agent.train_and_get_SER(n_episodes = 256,\n",
    "                        n_iter_per_EsN0dB = 1,\n",
    "                        n_training_iter = 60,\n",
    "                        min_EbN0dB = 0,\n",
    "                        max_EbN0dB = 10,\n",
    "                        EbN0dB_step = 1)\n",
    "agent.env.visualize()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f7be9d51035affafb29dceabe234970f757e220d15705efccd7c466e6060a35"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
